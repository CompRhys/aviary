{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from torch import __version__ as TORCH_VERSION\n",
    "!pip install pymatgen pybtex retrying  # install requirements to query sample data\n",
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{TORCH_VERSION}.html  # install torch scatter for aviary\n",
    "!pip install -U git+https://github.com/CompRhys/aviary.git  # install aviary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from pymatgen.ext.optimade import OptimadeRester\n",
    "\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "\n",
    "from aviary.utils import results_multitask, train_ensemble\n",
    "from aviary.wren.utils import get_aflow_label_spglib, count_wyks\n",
    "\n",
    "from aviary.roost.model import Roost\n",
    "from aviary.roost.data import CompositionData, collate_batch as roost_cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = OptimadeRester([\"mp\",])\n",
    "results = opt.get_structures(elements=[\"N\"], nelements=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "df.rename(columns={\"mp\": \"final_structure\"}, inplace=True)\n",
    "df[\"composition\"] = df[\"final_structure\"].apply(lambda x: x.composition.reduced_formula)\n",
    "df[\"volume_per_atom\"] = df[\"final_structure\"].apply(lambda x: x.volume/len(x))\n",
    "df[\"wyckoff\"] = df[\"final_structure\"].apply(get_aflow_label_spglib)\n",
    "\n",
    "df = df[df[\"wyckoff\"].apply(count_wyks) < 16]\n",
    "df = df[df[\"final_structure\"].apply(len) < 64]\n",
    "df = df[df[\"volume_per_atom\"] < 500]\n",
    "\n",
    "df.index.name = \"material_id\"\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = False\n",
    "fine_tune = None\n",
    "transfer = None\n",
    "\n",
    "optim = \"AdamW\"\n",
    "learning_rate = 3e-4\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-6\n",
    "batch_size = 128\n",
    "workers = 0  \n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "targets = [\"volume_per_atom\"]\n",
    "tasks = [\"regression\"]\n",
    "losses = [\"L1\"]\n",
    "robust = True\n",
    "\n",
    "data_seed = 42\n",
    "test_size = 0.2\n",
    "sample = 1\n",
    "\n",
    "ensemble = 1\n",
    "run_id = 1\n",
    "epochs = 100\n",
    "log = False\n",
    "\n",
    "# NOTE setting workers to zero means that the data is loaded in the main\n",
    "# process and enables caching\n",
    "\n",
    "data_params = {\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_workers\": workers,\n",
    "    \"pin_memory\": False,\n",
    "    \"shuffle\": True,\n",
    "}\n",
    "\n",
    "setup_params = {\n",
    "    \"optim\": optim,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"momentum\": momentum,\n",
    "    \"device\": device,\n",
    "}\n",
    "\n",
    "restart_params = {\n",
    "    \"resume\": resume,\n",
    "    \"fine_tune\": fine_tune,\n",
    "    \"transfer\": transfer,\n",
    "}\n",
    "\n",
    "task_dict = dict(zip(targets, tasks))\n",
    "loss_dict = dict(zip(targets, losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)  # ensure reproducible results\n",
    "\n",
    "elem_emb = \"matscholar200\"\n",
    "model_name = \"roost-reg-test\"\n",
    "\n",
    "data_params[\"collate_fn\"] = roost_cb\n",
    "data_params[\"shuffle\"] = True \n",
    "\n",
    "dataset = CompositionData(\n",
    "    df=df,\n",
    "    elem_emb=elem_emb,\n",
    "    task_dict=task_dict, \n",
    ")\n",
    "n_targets = dataset.n_targets\n",
    "elem_emb_len = dataset.elem_emb_len\n",
    "\n",
    "train_idx = list(range(len(dataset)))\n",
    "\n",
    "print(f\"using {test_size} of training set as test set\")\n",
    "train_idx, test_idx = split(train_idx, random_state=data_seed, test_size=test_size)\n",
    "test_set = torch.utils.data.Subset(dataset, test_idx)\n",
    "\n",
    "print(\"No validation set used, using test set for evaluation purposes\")\n",
    "# NOTE that when using this option care must be taken not to\n",
    "# peak at the test-set. The only valid model to use is the one\n",
    "# obtained after the final epoch where the epoch count is\n",
    "# decided in advance of the experiment.\n",
    "val_set = test_set\n",
    "\n",
    "train_set = torch.utils.data.Subset(dataset, train_idx[0::sample])\n",
    "\n",
    "model_params = {\n",
    "    \"task_dict\": task_dict,\n",
    "    \"robust\": robust,\n",
    "    \"n_targets\": n_targets,\n",
    "    \"elem_emb_len\": elem_emb_len,\n",
    "    \"elem_fea_len\": 64,\n",
    "    \"n_graph\": 3,\n",
    "    \"elem_heads\": 3,\n",
    "    \"elem_gate\": [256],\n",
    "    \"elem_msg\": [256],\n",
    "    \"cry_heads\": 3,\n",
    "    \"cry_gate\": [256],\n",
    "    \"cry_msg\": [256],\n",
    "    \"trunk_hidden\": [128, 128],\n",
    "    \"out_hidden\": [64, 64],\n",
    "}\n",
    "\n",
    "train_ensemble(\n",
    "    model_class=Roost,\n",
    "    model_name=model_name,\n",
    "    run_id=run_id,\n",
    "    ensemble_folds=ensemble,\n",
    "    epochs=epochs,\n",
    "    train_set=train_set,\n",
    "    val_set=val_set,\n",
    "    log=log,\n",
    "    data_params=data_params,\n",
    "    setup_params=setup_params,\n",
    "    restart_params=restart_params,\n",
    "    model_params=model_params,\n",
    "    loss_dict=loss_dict,\n",
    ")\n",
    "\n",
    "data_params[\"shuffle\"] = False  # need fixed data order due to ensembling\n",
    "\n",
    "roost_results_dict = results_multitask(\n",
    "    model_class=Roost,\n",
    "    model_name=model_name,\n",
    "    run_id=run_id,\n",
    "    ensemble_folds=ensemble,\n",
    "    test_set=test_set,\n",
    "    data_params=data_params,\n",
    "    robust=robust,\n",
    "    task_dict=task_dict,\n",
    "    device=device,\n",
    "    eval_type=\"checkpoint\",\n",
    "    save_results=False,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
