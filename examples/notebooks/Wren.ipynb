{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    %%capture\n",
    "    from torch import __version__ as TORCH_VERSION\n",
    "\n",
    "    print(f\"{TORCH_VERSION=}\")\n",
    "\n",
    "    !pip install -U git+https://github.com/CompRhys/aviary.git  # install aviary\n",
    "    !wget -O taata.json.gz https://figshare.com/ndownloader/files/34423997"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pymatgen.analysis.prototypes import (\n",
    "    count_wyckoff_positions,\n",
    "    get_protostructure_label_from_spglib,\n",
    ")\n",
    "from pymatgen.core import Structure\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from aviary.utils import results_multitask, train_ensemble\n",
    "from aviary.wren.data import WyckoffData\n",
    "from aviary.wren.data import collate_batch as wren_cb\n",
    "from aviary.wren.model import Wren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spglib: ssm_get_exact_positions failed.\n",
      "spglib: get_bravais_exact_positions_and_lattice failed.\n",
      "spglib: ssm_get_exact_positions failed.\n",
      "spglib: get_bravais_exact_positions_and_lattice failed.\n",
      "spglib: ssm_get_exact_positions failed.\n",
      "spglib: get_bravais_exact_positions_and_lattice failed.\n",
      "spglib: ssm_get_exact_positions failed.\n",
      "spglib: get_bravais_exact_positions_and_lattice failed.\n",
      "spglib: ssm_get_exact_positions failed.\n",
      "spglib: get_bravais_exact_positions_and_lattice failed.\n",
      "spglib: ssm_get_exact_positions failed.\n",
      "spglib: get_bravais_exact_positions_and_lattice failed.\n",
      "spglib: ssm_get_exact_positions failed.\n",
      "spglib: get_bravais_exact_positions_and_lattice failed.\n",
      "spglib: ssm_get_exact_positions failed.\n",
      "spglib: get_bravais_exact_positions_and_lattice failed.\n",
      "spglib: ssm_get_exact_positions failed.\n",
      "spglib: get_bravais_exact_positions_and_lattice failed.\n",
      "spglib: ssm_get_exact_positions failed.\n",
      "spglib: get_bravais_exact_positions_and_lattice failed.\n",
      "spglib: ssm_get_exact_positions failed.\n",
      "spglib: get_bravais_exact_positions_and_lattice failed.\n",
      "spglib: ssm_get_exact_positions failed.\n",
      "spglib: get_bravais_exact_positions_and_lattice failed.\n",
      "spglib: ssm_get_exact_positions failed.\n",
      "spglib: get_bravais_exact_positions_and_lattice failed.\n",
      "spglib: ssm_get_exact_positions failed.\n",
      "spglib: get_bravais_exact_positions_and_lattice failed.\n",
      "spglib: ssm_get_exact_positions failed.\n",
      "spglib: get_bravais_exact_positions_and_lattice failed.\n",
      "spglib: ssm_get_exact_positions failed.\n",
      "spglib: get_bravais_exact_positions_and_lattice failed.\n",
      "spglib: ssm_get_exact_positions failed.\n",
      "spglib: get_bravais_exact_positions_and_lattice failed.\n",
      "spglib: ssm_get_exact_positions failed.\n",
      "spglib: get_bravais_exact_positions_and_lattice failed.\n",
      "spglib: ssm_get_exact_positions failed.\n",
      "spglib: get_bravais_exact_positions_and_lattice failed.\n",
      "spglib: ssm_get_exact_positions failed.\n",
      "spglib: get_bravais_exact_positions_and_lattice failed.\n",
      "spglib: ssm_get_exact_positions failed.\n",
      "spglib: get_bravais_exact_positions_and_lattice failed.\n",
      "spglib: ssm_get_exact_positions failed.\n",
      "spglib: get_bravais_exact_positions_and_lattice failed.\n",
      "spglib: ssm_get_exact_positions failed.\n",
      "spglib: get_bravais_exact_positions_and_lattice failed.\n",
      "spglib: ssm_get_exact_positions failed.\n",
      "spglib: get_bravais_exact_positions_and_lattice failed.\n"
     ]
    }
   ],
   "source": [
    "with gzip.open(\"taata.json.gz\", \"r\") as fin:\n",
    "    json_bytes = fin.read()\n",
    "\n",
    "json_str = json_bytes.decode(\"utf-8\")\n",
    "data = json.loads(json_str)\n",
    "\n",
    "df = pd.DataFrame(data[\"data\"], columns=data[\"columns\"])\n",
    "\n",
    "df[\"final_structure\"] = [Structure.from_dict(x) for x in df.final_structure]\n",
    "\n",
    "df[\"composition\"] = [x.composition.reduced_formula for x in df.final_structure]\n",
    "df[\"volume_per_atom\"] = [x.volume / len(x) for x in df.final_structure]\n",
    "df[\"protostructure\"] = df[\"final_structure\"].map(get_protostructure_label_from_spglib)\n",
    "\n",
    "df = df[df.protostructure.map(count_wyckoff_positions) < 16]\n",
    "df[\"n_sites\"] = df.final_structure.map(len)\n",
    "df = df[df.n_sites < 64]\n",
    "df = df[df.volume_per_atom < 500]\n",
    "\n",
    "# NOTE for roost we keep only the lowest lying structures for each composition\n",
    "df = df.sort_values([\"protostructure\", \"E_vasp_per_atom\"]).drop_duplicates(\n",
    "    \"protostructure\", keep=\"first\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = False\n",
    "fine_tune = None\n",
    "transfer = None\n",
    "\n",
    "optim = \"AdamW\"\n",
    "learning_rate = 3e-4\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-6\n",
    "batch_size = 128\n",
    "workers = 0\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "targets = [\"E_vasp_per_atom\"]\n",
    "tasks = [\"regression\"]\n",
    "losses = [\"L1\"]\n",
    "robust = True\n",
    "\n",
    "data_seed = 42\n",
    "test_size = 0.2\n",
    "sample = 1\n",
    "\n",
    "ensemble = 1\n",
    "run_id = 1\n",
    "epochs = 100\n",
    "log = False\n",
    "\n",
    "# NOTE setting workers to zero means that the data is loaded in the main\n",
    "# process and enables caching\n",
    "\n",
    "data_params = {\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_workers\": workers,\n",
    "    \"pin_memory\": False,\n",
    "    \"shuffle\": True,\n",
    "}\n",
    "\n",
    "setup_params = {\n",
    "    \"optim\": optim,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"momentum\": momentum,\n",
    "    \"device\": device,\n",
    "}\n",
    "\n",
    "restart_params = {\n",
    "    \"resume\": resume,\n",
    "    \"fine_tune\": fine_tune,\n",
    "    \"transfer\": transfer,\n",
    "}\n",
    "\n",
    "task_dict = dict(zip(targets, tasks, strict=False))\n",
    "loss_dict = dict(zip(targets, losses, strict=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using 0.2 of training set as test set\n",
      "No validation set used, using test set for evaluation purposes\n",
      "Total Number of Trainable Parameters: 1,154,330\n",
      "Dummy MAE: 0.9670\n",
      "Epoch: [0/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.95      Loss 1.11      RMSE 1.19     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.84      Loss 0.97      RMSE 1.07     \n",
      "Epoch: [1/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.54      Loss 0.52      RMSE 0.73     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.36      Loss 0.18      RMSE 0.50     \n",
      "Epoch: [2/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.35      Loss 0.13      RMSE 0.48     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.34      Loss 0.07      RMSE 0.47     \n",
      "Epoch: [3/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.32      Loss 0.01      RMSE 0.44     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.32      Loss 0.01      RMSE 0.44     \n",
      "Epoch: [4/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.31      Loss -0.02     RMSE 0.43     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.34      Loss 0.08      RMSE 0.47     \n",
      "Epoch: [5/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.30      Loss -0.04     RMSE 0.42     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.30      Loss -0.06     RMSE 0.41     \n",
      "Epoch: [6/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.28      Loss -0.12     RMSE 0.40     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.29      Loss -0.10     RMSE 0.41     \n",
      "Epoch: [7/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.27      Loss -0.15     RMSE 0.39     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.29      Loss -0.10     RMSE 0.41     \n",
      "Epoch: [8/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.27      Loss -0.15     RMSE 0.39     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.30      Loss -0.07     RMSE 0.43     \n",
      "Epoch: [9/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.26      Loss -0.19     RMSE 0.38     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.27      Loss -0.16     RMSE 0.40     \n",
      "Epoch: [10/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.25      Loss -0.25     RMSE 0.36     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.27      Loss -0.18     RMSE 0.39     \n",
      "Epoch: [11/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.25      Loss -0.27     RMSE 0.36     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.29      Loss -0.10     RMSE 0.39     \n",
      "Epoch: [12/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.24      Loss -0.28     RMSE 0.35     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.26      Loss -0.21     RMSE 0.37     \n",
      "Epoch: [13/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.24      Loss -0.31     RMSE 0.35     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.26      Loss -0.22     RMSE 0.37     \n",
      "Epoch: [14/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.24      Loss -0.31     RMSE 0.34     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.28      Loss -0.10     RMSE 0.40     \n",
      "Epoch: [15/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.24      Loss -0.30     RMSE 0.34     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.29      Loss -0.07     RMSE 0.41     \n",
      "Epoch: [16/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.24      Loss -0.30     RMSE 0.34     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.27      Loss -0.18     RMSE 0.38     \n",
      "Epoch: [17/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.22      Loss -0.40     RMSE 0.33     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.27      Loss -0.13     RMSE 0.37     \n",
      "Epoch: [18/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.22      Loss -0.41     RMSE 0.32     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.24      Loss -0.27     RMSE 0.36     \n",
      "Epoch: [19/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.22      Loss -0.42     RMSE 0.32     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.25      Loss -0.27     RMSE 0.36     \n",
      "Epoch: [20/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.22      Loss -0.40     RMSE 0.32     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.24      Loss -0.30     RMSE 0.35     \n",
      "Epoch: [21/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.21      Loss -0.46     RMSE 0.31     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.23      Loss -0.32     RMSE 0.34     \n",
      "Epoch: [22/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.21      Loss -0.48     RMSE 0.31     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.24      Loss -0.29     RMSE 0.35     \n",
      "Epoch: [23/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.20      Loss -0.52     RMSE 0.30     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.25      Loss -0.17     RMSE 0.35     \n",
      "Epoch: [24/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.20      Loss -0.53     RMSE 0.30     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.23      Loss -0.33     RMSE 0.34     \n",
      "Epoch: [25/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.20      Loss -0.53     RMSE 0.30     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.23      Loss -0.30     RMSE 0.34     \n",
      "Epoch: [26/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.20      Loss -0.53     RMSE 0.30     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.23      Loss -0.34     RMSE 0.33     \n",
      "Epoch: [27/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.20      Loss -0.55     RMSE 0.29     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.23      Loss -0.31     RMSE 0.34     \n",
      "Epoch: [28/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.19      Loss -0.56     RMSE 0.29     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.23      Loss -0.29     RMSE 0.33     \n",
      "Epoch: [29/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.19      Loss -0.58     RMSE 0.29     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.22      Loss -0.34     RMSE 0.33     \n",
      "Epoch: [30/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.20      Loss -0.55     RMSE 0.29     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.23      Loss -0.34     RMSE 0.33     \n",
      "Epoch: [31/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.19      Loss -0.57     RMSE 0.29     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.23      Loss -0.35     RMSE 0.33     \n",
      "Epoch: [32/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.19      Loss -0.61     RMSE 0.28     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.23      Loss -0.31     RMSE 0.33     \n",
      "Epoch: [33/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.19      Loss -0.62     RMSE 0.28     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.23      Loss -0.28     RMSE 0.33     \n",
      "Epoch: [34/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.19      Loss -0.60     RMSE 0.28     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.22      Loss -0.32     RMSE 0.32     \n",
      "Epoch: [35/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.18      Loss -0.62     RMSE 0.28     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.29      Loss 0.11      RMSE 0.40     \n",
      "Epoch: [36/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.18      Loss -0.62     RMSE 0.28     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.24      Loss -0.21     RMSE 0.35     \n",
      "Epoch: [37/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.18      Loss -0.64     RMSE 0.28     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.22      Loss -0.34     RMSE 0.32     \n",
      "Epoch: [38/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.18      Loss -0.63     RMSE 0.28     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.22      Loss -0.35     RMSE 0.33     \n",
      "Epoch: [39/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.18      Loss -0.66     RMSE 0.27     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.22      Loss -0.36     RMSE 0.32     \n",
      "Epoch: [40/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.18      Loss -0.69     RMSE 0.27     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.22      Loss -0.34     RMSE 0.33     \n",
      "Epoch: [41/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.18      Loss -0.68     RMSE 0.27     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.22      Loss -0.36     RMSE 0.32     \n",
      "Epoch: [42/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.17      Loss -0.70     RMSE 0.27     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.22      Loss -0.33     RMSE 0.32     \n",
      "Epoch: [43/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.17      Loss -0.74     RMSE 0.27     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.21      Loss -0.35     RMSE 0.32     \n",
      "Epoch: [44/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.17      Loss -0.72     RMSE 0.27     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.22      Loss -0.36     RMSE 0.32     \n",
      "Epoch: [45/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.17      Loss -0.74     RMSE 0.26     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.21      Loss -0.40     RMSE 0.32     \n",
      "Epoch: [46/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.17      Loss -0.76     RMSE 0.26     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.22      Loss -0.35     RMSE 0.31     \n",
      "Epoch: [47/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.17      Loss -0.70     RMSE 0.27     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.21      Loss -0.39     RMSE 0.31     \n",
      "Epoch: [48/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.17      Loss -0.71     RMSE 0.27     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.22      Loss -0.31     RMSE 0.33     \n",
      "Epoch: [49/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.17      Loss -0.73     RMSE 0.26     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.21      Loss -0.38     RMSE 0.31     \n",
      "Epoch: [50/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.16      Loss -0.79     RMSE 0.26     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.20      Loss -0.41     RMSE 0.31     \n",
      "Epoch: [51/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.17      Loss -0.74     RMSE 0.26     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.22      Loss -0.36     RMSE 0.32     \n",
      "Epoch: [52/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.17      Loss -0.77     RMSE 0.26     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.21      Loss -0.36     RMSE 0.31     \n",
      "Epoch: [53/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.16      Loss -0.78     RMSE 0.26     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.21      Loss -0.40     RMSE 0.31     \n",
      "Epoch: [54/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.16      Loss -0.79     RMSE 0.26     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.21      Loss -0.33     RMSE 0.32     \n",
      "Epoch: [55/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.17      Loss -0.76     RMSE 0.26     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.21      Loss -0.40     RMSE 0.31     \n",
      "Epoch: [56/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.16      Loss -0.82     RMSE 0.25     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.21      Loss -0.39     RMSE 0.31     \n",
      "Epoch: [57/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.16      Loss -0.81     RMSE 0.26     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.21      Loss -0.40     RMSE 0.31     \n",
      "Epoch: [58/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.16      Loss -0.81     RMSE 0.26     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.21      Loss -0.43     RMSE 0.32     \n",
      "Epoch: [59/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.17      Loss -0.73     RMSE 0.26     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.21      Loss -0.42     RMSE 0.31     \n",
      "Epoch: [60/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.16      Loss -0.82     RMSE 0.26     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.21      Loss -0.38     RMSE 0.31     \n",
      "Epoch: [61/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.16      Loss -0.79     RMSE 0.26     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.21      Loss -0.41     RMSE 0.31     \n",
      "Epoch: [62/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.15      Loss -0.87     RMSE 0.25     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.20      Loss -0.41     RMSE 0.30     \n",
      "Epoch: [63/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.16      Loss -0.85     RMSE 0.25     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.20      Loss -0.39     RMSE 0.31     \n",
      "Epoch: [64/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.15      Loss -0.87     RMSE 0.25     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.20      Loss -0.41     RMSE 0.31     \n",
      "Epoch: [65/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.15      Loss -0.86     RMSE 0.25     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.20      Loss -0.36     RMSE 0.30     \n",
      "Epoch: [66/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.15      Loss -0.87     RMSE 0.25     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.20      Loss -0.42     RMSE 0.30     \n",
      "Epoch: [67/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.15      Loss -0.87     RMSE 0.25     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.20      Loss -0.40     RMSE 0.31     \n",
      "Epoch: [68/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.15      Loss -0.88     RMSE 0.25     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.20      Loss -0.34     RMSE 0.30     \n",
      "Epoch: [69/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.15      Loss -0.88     RMSE 0.25     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.22      Loss -0.23     RMSE 0.33     \n",
      "Epoch: [70/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.15      Loss -0.89     RMSE 0.25     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.23      Loss 0.01      RMSE 0.33     \n",
      "Epoch: [71/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.17      Loss -0.74     RMSE 0.26     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.21      Loss -0.32     RMSE 0.31     \n",
      "Epoch: [72/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.15      Loss -0.92     RMSE 0.24     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.20      Loss -0.46     RMSE 0.30     \n",
      "Epoch: [73/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.15      Loss -0.94     RMSE 0.24     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.20      Loss -0.28     RMSE 0.31     \n",
      "Epoch: [74/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.15      Loss -0.90     RMSE 0.25     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.20      Loss -0.37     RMSE 0.30     \n",
      "Epoch: [75/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.15      Loss -0.87     RMSE 0.24     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.21      Loss -0.28     RMSE 0.31     \n",
      "Epoch: [76/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.15      Loss -0.90     RMSE 0.24     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.20      Loss -0.41     RMSE 0.30     \n",
      "Epoch: [77/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.15      Loss -0.91     RMSE 0.24     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.20      Loss -0.46     RMSE 0.30     \n",
      "Epoch: [78/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.15      Loss -0.94     RMSE 0.24     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.20      Loss -0.32     RMSE 0.30     \n",
      "Epoch: [79/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.14      Loss -0.98     RMSE 0.24     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.20      Loss -0.29     RMSE 0.31     \n",
      "Epoch: [80/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.14      Loss -0.96     RMSE 0.24     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.21      Loss -0.24     RMSE 0.31     \n",
      "Epoch: [81/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.15      Loss -0.91     RMSE 0.24     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.20      Loss -0.38     RMSE 0.31     \n",
      "Epoch: [82/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.14      Loss -0.96     RMSE 0.24     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.20      Loss -0.38     RMSE 0.31     \n",
      "Epoch: [83/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.15      Loss -0.94     RMSE 0.24     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.20      Loss -0.38     RMSE 0.31     \n",
      "Epoch: [84/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.14      Loss -0.97     RMSE 0.24     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.20      Loss -0.34     RMSE 0.30     \n",
      "Epoch: [85/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.14      Loss -0.99     RMSE 0.24     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.20      Loss -0.31     RMSE 0.31     \n",
      "Epoch: [86/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.14      Loss -1.00     RMSE 0.24     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.20      Loss -0.36     RMSE 0.30     \n",
      "Epoch: [87/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.15      Loss -0.92     RMSE 0.24     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.19      Loss -0.41     RMSE 0.30     \n",
      "Epoch: [88/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.14      Loss -1.02     RMSE 0.23     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.20      Loss -0.30     RMSE 0.30     \n",
      "Epoch: [89/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.14      Loss -0.95     RMSE 0.24     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.20      Loss -0.31     RMSE 0.30     \n",
      "Epoch: [90/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.14      Loss -1.01     RMSE 0.23     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.19      Loss -0.35     RMSE 0.30     \n",
      "Epoch: [91/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.14      Loss -1.01     RMSE 0.23     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.19      Loss -0.38     RMSE 0.30     \n",
      "Epoch: [92/99]\n",
      "    train: E_vasp_per_atom N 57 MAE 0.14      Loss -1.03     RMSE 0.23     \n",
      " evaluate: E_vasp_per_atom N 1 MAE 0.20      Loss -0.32     RMSE 0.30     \n",
      "Epoch: [93/99]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)  # ensure reproducible results\n",
    "\n",
    "elem_embedding = \"matscholar200\"\n",
    "sym_embedding = \"bra-alg-off\"\n",
    "model_name = \"wren-reg-test\"\n",
    "\n",
    "data_params[\"collate_fn\"] = wren_cb\n",
    "data_params[\"shuffle\"] = True\n",
    "\n",
    "dataset = WyckoffData(df=df, task_dict=task_dict)\n",
    "n_targets = dataset.n_targets\n",
    "\n",
    "train_idx = list(range(len(dataset)))\n",
    "\n",
    "print(f\"using {test_size} of training set as test set\")\n",
    "train_idx, test_idx = split(train_idx, random_state=data_seed, test_size=test_size)\n",
    "test_set = torch.utils.data.Subset(dataset, test_idx)\n",
    "\n",
    "print(\"No validation set used, using test set for evaluation purposes\")\n",
    "# NOTE that when using this option care must be taken not to\n",
    "# peak at the test-set. The only valid model to use is the one\n",
    "# obtained after the final epoch where the epoch count is\n",
    "# decided in advance of the experiment.\n",
    "val_set = test_set\n",
    "\n",
    "train_set = torch.utils.data.Subset(dataset, train_idx[0::sample])\n",
    "\n",
    "train_loader = DataLoader(train_set, **data_params)\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    **{**data_params, \"batch_size\": 16 * data_params[\"batch_size\"], \"shuffle\": False},\n",
    ")\n",
    "\n",
    "model_params = {\n",
    "    \"task_dict\": task_dict,\n",
    "    \"robust\": robust,\n",
    "    \"n_targets\": n_targets,\n",
    "    \"elem_embedding\": elem_embedding,\n",
    "    \"elem_fea_len\": 32,\n",
    "    \"sym_embedding\": sym_embedding,\n",
    "    \"sym_fea_len\": 32,\n",
    "    \"n_graph\": 3,\n",
    "    \"elem_heads\": 1,\n",
    "    \"elem_gate\": [256],\n",
    "    \"elem_msg\": [256],\n",
    "    \"cry_heads\": 1,\n",
    "    \"cry_gate\": [256],\n",
    "    \"cry_msg\": [256],\n",
    "    \"trunk_hidden\": [128, 128],\n",
    "    \"out_hidden\": [64, 64],\n",
    "}\n",
    "\n",
    "train_ensemble(\n",
    "    model_class=Wren,\n",
    "    model_name=model_name,\n",
    "    run_id=run_id,\n",
    "    ensemble_folds=ensemble,\n",
    "    epochs=epochs,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    log=log,\n",
    "    setup_params=setup_params,\n",
    "    restart_params=restart_params,\n",
    "    model_params=model_params,\n",
    "    loss_dict=loss_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "------------Evaluate model on Test Set------------\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Evaluating Model\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "task_dict {'last phdos peak': 'regression'} of checkpoint resume='/Users/radical-rhys/Radical/aviary/models/wren-reg-test/checkpoint-r1.pth.tar' does not match provided task_dict={'E_vasp_per_atom': 'regression'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m      2\u001b[0m     test_set,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata_params, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m64\u001b[39m \u001b[38;5;241m*\u001b[39m data_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshuffle\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m},\n\u001b[1;32m      4\u001b[0m )\n\u001b[0;32m----> 6\u001b[0m roost_results_dict \u001b[38;5;241m=\u001b[39m \u001b[43mresults_multitask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mWren\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensemble_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensemble\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrobust\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrobust\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Radical/aviary/aviary/utils.py:490\u001b[0m, in \u001b[0;36mresults_multitask\u001b[0;34m(model_class, model_name, run_id, ensemble_folds, test_loader, robust, task_dict, device, eval_type, print_results, save_results)\u001b[0m\n\u001b[1;32m    488\u001b[0m chkpt_task_dict \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_params\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chkpt_task_dict \u001b[38;5;241m!=\u001b[39m task_dict:\n\u001b[0;32m--> 490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    491\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_dict \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchkpt_task_dict\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of checkpoint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresume\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m does not match \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    492\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprovided \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_dict\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    493\u001b[0m     )\n\u001b[1;32m    495\u001b[0m model: BaseModelClass \u001b[38;5;241m=\u001b[39m model_class(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheckpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_params\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    496\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mValueError\u001b[0m: task_dict {'last phdos peak': 'regression'} of checkpoint resume='/Users/radical-rhys/Radical/aviary/models/wren-reg-test/checkpoint-r1.pth.tar' does not match provided task_dict={'E_vasp_per_atom': 'regression'}"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    **{**data_params, \"batch_size\": 64 * data_params[\"batch_size\"], \"shuffle\": False},\n",
    ")\n",
    "\n",
    "roost_results_dict = results_multitask(\n",
    "    model_class=Wren,\n",
    "    model_name=model_name,\n",
    "    run_id=run_id,\n",
    "    ensemble_folds=ensemble,\n",
    "    test_loader=test_loader,\n",
    "    robust=robust,\n",
    "    task_dict=task_dict,\n",
    "    device=device,\n",
    "    eval_type=\"checkpoint\",\n",
    "    save_results=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "8022b3e932e045c760cb4633b91dd1cb8bc60d104ca9808334cbd1645adbe837"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
